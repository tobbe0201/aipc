"""
Advanced Q-learning implementering f√∂r AI Desktop Controller

Detta modul tillhandah√•ller avancerade reinforcement learning-funktioner 
med Deep Q-Networks (DQN), experience replay och target networks.
"""

import os
import numpy as np
import random
import time
import json
import logging
import pickle
from collections import deque
from datetime import datetime

# Kontrollera om vi har GPU-st√∂d
try:
    import torch
    import torch.nn as nn
    import torch.optim as optim
    import torch.nn.functional as F
    USE_CUDA = torch.cuda.is_available()
    DEVICE = torch.device('cuda' if USE_CUDA else 'cpu')
    print(f"üß† Neural networks kommer att k√∂ras p√•: {DEVICE}")
except ImportError:
    print("‚ö†Ô∏è PyTorch √§r inte installerat. Faller tillbaka p√• numpy-baserad implementation.")
    USE_CUDA = False
    

# Konfigurera loggning
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler("qlearning.log"),
        logging.StreamHandler()
    ]
)

class ReplayBuffer:
    """Experience replay buffer f√∂r att lagra och sampla tidigare erfarenheter"""
    
    def __init__(self, capacity=10000):
        self.buffer = deque(maxlen=capacity)
        
    def add(self, state, action, reward, next_state, done):
        """L√§gg till en erfarenhet i buffern"""
        self.buffer.append((state, action, reward, next_state, done))
        
    def sample(self, batch_size):
        """Sampla en slumpm√§ssig batch av erfarenheter"""
        return random.sample(self.buffer, min(len(self.buffer), batch_size))
        
    def __len__(self):
        return len(self.buffer)
    
    def save(self, filename):
        """Spara buffern till fil"""
        with open(filename, 'wb') as f:
            pickle.dump(self.buffer, f)
        
    def load(self, filename):
        """Ladda buffern fr√•n fil"""
        if os.path.exists(filename):
            with open(filename, 'rb') as f:
                self.buffer = pickle.load(f)


class DQNModel:
    """Deep Q-Network modell"""
    
    def __init__(self, state_dim, action_dim, hidden_dim=128):
        if USE_CUDA:
            self.model = self._build_torch_model(state_dim, action_dim, hidden_dim)
            self.target_model = self._build_torch_model(state_dim, action_dim, hidden_dim)
            self.optimizer = optim.Adam(self.model.parameters(), lr=0.001)
            # Initiera target model med samma vikter
            self.update_target_model()
        else:
            # Numpy-baserad fallback implementation
            self.state_dim = state_dim
            self.action_dim = action_dim
            self.hidden_dim = hidden_dim
            # Enkla matriser f√∂r vikter i ett tv√•-lagers n√§tverk
            self.weights1 = np.random.randn(state_dim, hidden_dim) / np.sqrt(state_dim)
            self.weights2 = np.random.randn(hidden_dim, action_dim) / np.sqrt(hidden_dim)
            self.target_weights1 = self.weights1.copy()
            self.target_weights2 = self.weights2.copy()
            self.learning_rate = 0.001
    
    def _build_torch_model(self, state_dim, action_dim, hidden_dim):
        """Bygg PyTorch DQN-modell"""
        model = nn.Sequential(
            nn.Linear(state_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, hidden_dim),
            nn.ReLU(),
            nn.Linear(hidden_dim, action_dim)
        ).to(DEVICE)
        return model
    
    def predict(self, state, use_target=False):
        """F√∂ruts√§g Q-v√§rden f√∂r ett givet tillst√•nd"""
        if USE_CUDA:
            state_tensor = torch.FloatTensor(state).to(DEVICE)
            with torch.no_grad():
                if use_target:
                    return self.target_model(state_tensor).cpu().numpy()
                else:
                    return self.model(state_tensor).cpu().numpy()
        else:
            # Numpy forward pass
            if use_target:
                hidden = np.dot(state, self.target_weights1)
                hidden = np.maximum(0, hidden)  # ReLU
                return np.dot(hidden, self.target_weights2)
            else:
                hidden = np.dot(state, self.weights1)
                hidden = np.maximum(0, hidden)  # ReLU
                return np.dot(hidden, self.weights2)
    
    def train(self, states, actions, rewards, next_states, dones, gamma=0.99):
        """Tr√§na modellen p√• en batch av erfarenheter"""
        if USE_CUDA:
            # Konvertera till tensorer
            states_tensor = torch.FloatTensor(states).to(DEVICE)
            actions_tensor = torch.LongTensor(actions).to(DEVICE)
            rewards_tensor = torch.FloatTensor(rewards).to(DEVICE)
            next_states_tensor = torch.FloatTensor(next_states).to(DEVICE)
            dones_tensor = torch.FloatTensor(dones).to(DEVICE)
            
            # Ber√§kna f√∂rv√§ntade Q-v√§rden
            q_values = self.model(states_tensor).gather(1, actions_tensor.unsqueeze(1)).squeeze(1)
            
            # Ber√§kna n√§sta tillst√•nds Q-v√§rden med target_model
            with torch.no_grad():
                next_q_values = self.target_model(next_states_tensor).max(1)[0]
                expected_q_values = rewards_tensor + gamma * next_q_values * (1 - dones_tensor)
            
            # Ber√§kna loss och uppdatera modellen
            loss = F.mse_loss(q_values, expected_q_values)
            self.optimizer.zero_grad()
            loss.backward()
            self.optimizer.step()
            
            return loss.item()
        else:
            # Numpy training implementation
            batch_size = len(states)
            losses = []
            
            for i in range(batch_size):
                state = states[i]
                action = actions[i]
                reward = rewards[i]
                next_state = next_states[i]
                done = dones[i]
                
                # F√∂ruts√§g Q-v√§rden
                q_values = self.predict(state)
                
                # Ber√§kna target Q-v√§rde
                target_q = q_values.copy()
                if done:
                    target_q[action] = reward
                else:
                    next_q = self.predict(next_state, use_target=True)
                    target_q[action] = reward + gamma * np.max(next_q)
                
                # Ber√§kna gradient och uppdatera vikter (f√∂renklad)
                # F√∂rsta lagret
                hidden = np.dot(state, self.weights1)
                hidden_activated = np.maximum(0, hidden)  # ReLU
                
                # Forward pass
                output = np.dot(hidden_activated, self.weights2)
                
                # Error i output layer
                error = output - target_q
                loss = np.sum(error**2)
                losses.append(loss)
                
                # Backpropagation (f√∂renklad)
                d_output = error
                d_weights2 = np.outer(hidden_activated, d_output)
                
                # Uppdatera andra lagrets vikter
                self.weights2 -= self.learning_rate * d_weights2
                
                # Bak√•tproparera till f√∂rsta lagret
                d_hidden = np.dot(d_output, self.weights2.T)
                d_hidden[hidden <= 0] = 0  # ReLU-derivatan
                d_weights1 = np.outer(state, d_hidden)
                
                # Uppdatera f√∂rsta lagrets vikter
                self.weights1 -= self.learning_rate * d_weights1
            
            return np.mean(losses)
    
    def update_target_model(self):
        """Uppdatera target model med vikter fr√•n huvudmodellen"""
        if USE_CUDA:
            self.target_model.load_state_dict(self.model.state_dict())
        else:
            self.target_weights1 = self.weights1.copy()
            self.target_weights2 = self.weights2.copy()
    
    def save(self, filename):
        """Spara modellen till fil"""
        if USE_CUDA:
            torch.save({
                'model_state_dict': self.model.state_dict(),
                'target_model_state_dict': self.target_model.state_dict(),
                'optimizer_state_dict': self.optimizer.state_dict()
            }, filename)
        else:
            with open(filename, 'wb') as f:
                pickle.dump({
                    'weights1': self.weights1,
                    'weights2': self.weights2,
                    'target_weights1': self.target_weights1,
                    'target_weights2': self.target_weights2
                }, f)
    
    def load(self, filename):
        """Ladda modellen fr√•n fil"""
        if os.path.exists(filename):
            if USE_CUDA:
                checkpoint = torch.load(filename)
                self.model.load_state_dict(checkpoint['model_state_dict'])
                self.target_model.load_state_dict(checkpoint['target_model_state_dict'])
                self.optimizer.load_state_dict(checkpoint['optimizer_state_dict'])
                print(f"‚úÖ DQN-modell laddad fr√•n {filename}")
            else:
                with open(filename, 'rb') as f:
                    data = pickle.load(f)
                    self.weights1 = data['weights1']
                    self.weights2 = data['weights2']
                    self.target_weights1 = data['target_weights1']
                    self.target_weights2 = data['target_weights2']
                print(f"‚úÖ Numpy-modell laddad fr√•n {filename}")
            return True
        return False


class AdvancedQLearningAgent:
    """Advanced Q-learning agent med DQN, experience replay och target networks"""
    
    def __init__(self, state_features, action_space, 
                 learning_rate=0.001, gamma=0.99, 
                 epsilon_start=1.0, epsilon_min=0.1, epsilon_decay=0.995,
                 buffer_size=10000, batch_size=64, target_update_freq=10):
        """
        Initiera AdvancedQLearningAgent
        
        Args:
            state_features: Lista med funktioner/egenskaper i tillst√•ndet
            action_space: Lista med m√∂jliga handlingar
            learning_rate: Inl√§rningshastighet
            gamma: Diskonteringsfaktor f√∂r framtida bel√∂ningar
            epsilon_start: Startv√§rde f√∂r utforskningssannolikhet
            epsilon_min: Minimiv√§rde f√∂r utforskningssannolikhet
            epsilon_decay: Minskningstakt f√∂r utforskningssannolikhet
            buffer_size: Storlek p√• experience replay buffer
            batch_size: Antal erfarenheter att tr√§na p√• i varje batch
            target_update_freq: Hur ofta (antal tr√§ningssteg) target network uppdateras
        """
        # M√∂jligheten att representera tillst√•nd och handlingar
        self.state_features = state_features
        self.state_dim = len(state_features)
        self.action_space = action_space
        self.action_dim = len(action_space)
        
        # Hyperparametrar
        self.gamma = gamma
        self.epsilon = epsilon_start
        self.epsilon_min = epsilon_min
        self.epsilon_decay = epsilon_decay
        self.batch_size = batch_size
        self.target_update_freq = target_update_freq
        
        # Skapa DQN-modell
        self.model = DQNModel(self.state_dim, self.action_dim)
        
        # Experience replay buffer
        self.replay_buffer = ReplayBuffer(capacity=buffer_size)
        
        # Tr√§ningsstatistik
        self.training_stats = {
            'episodes': 0,
            'steps': 0,
            'rewards': [],
            'losses': [],
            'epsilons': []
        }
        
        # Skapa mappar f√∂r att spara modeller och statistik
        os.makedirs("data/models", exist_ok=True)
        os.makedirs("data/stats", exist_ok=True)
        
        logging.info(f"‚úÖ Avancerad Q-learning agent initialiserad med {self.state_dim} state features och {self.action_dim} handlingar")
    
    def select_action(self, state, explore=True):
        """V√§lj handling baserat p√• epsilon-greedy policy"""
        if explore and random.random() < self.epsilon:
            # Utforska: v√§lj en slumpm√§ssig handling
            return random.randint(0, self.action_dim - 1)
        else:
            # Utnyttja: v√§lj handlingen med h√∂gst Q-v√§rde
            q_values = self.model.predict(state)
            return np.argmax(q_values)
    
    def get_action_from_index(self, action_idx):
        """Konvertera handling-index till faktisk handling"""
        if 0 <= action_idx < len(self.action_space):
            return self.action_space[action_idx]
        return None
    
    def get_action_index(self, action):
        """Konvertera handling till index"""
        try:
            return self.action_space.index(action)
        except ValueError:
            return -1
    
    def remember(self, state, action, reward, next_state, done):
        """Lagra erfarenhet i replay buffer"""
        self.replay_buffer.add(state, action, reward, next_state, done)
    
    def train(self):
        """Tr√§na modellen p√• en batch av erfarenheter fr√•n replay buffer"""
        if len(self.replay_buffer) < self.batch_size:
            return 0
        
        # Sampla en batch fr√•n replay buffer
        experiences = self.replay_buffer.sample(self.batch_size)
        
        # F√∂rbered data f√∂r tr√§ning
        states = np.array([exp[0] for exp in experiences])
        actions = np.array([exp[1] for exp in experiences])
        rewards = np.array([exp[2] for exp in experiences])
        next_states = np.array([exp[3] for exp in experiences])
        dones = np.array([exp[4] for exp in experiences], dtype=np.float32)
        
        # Tr√§na modellen
        loss = self.model.train(states, actions, rewards, next_states, dones, self.gamma)
        
        # Uppdatera tr√§ningsstatistik
        self.training_stats['steps'] += 1
        self.training_stats['losses'].append(loss)
        
        # Uppdatera target model periodiskt
        if self.training_stats['steps'] % self.target_update_freq == 0:
            self.model.update_target_model()
            logging.info("üîÑ Target network uppdaterad")
        
        return loss
    
    def update_epsilon(self):
        """Uppdatera epsilon (utforskningssannolikhet)"""
        if self.epsilon > self.epsilon_min:
            self.epsilon *= self.epsilon_decay
        self.training_stats['epsilons'].append(self.epsilon)
    
    def end_episode(self, total_reward):
        """Slutf√∂r en episode och uppdatera statistik"""
        self.training_stats['episodes'] += 1
        self.training_stats['rewards'].append(total_reward)
        self.update_epsilon()
        
        # Logga statistik var 10:e episode
        if self.training_stats['episodes'] % 10 == 0:
            recent_rewards = self.training_stats['rewards'][-10:]
            avg_reward = sum(recent_rewards) / len(recent_rewards)
            logging.info(f"üìä Episode {self.training_stats['episodes']}: "
                        f"Avg bel√∂ning={avg_reward:.2f}, "
                        f"Epsilon={self.epsilon:.4f}")
        
        # Spara modell och statistik var 100:e episode
        if self.training_stats['episodes'] % 100 == 0:
            self.save_model()
            self.save_stats()
    
    def save_model(self, filename=None):
        """Spara modell till fil"""
        if filename is None:
            filename = f"data/models/dqn_model_{int(time.time())}.pkl"
        
        self.model.save(filename)
        logging.info(f"üíæ Modell sparad till {filename}")
    
    def load_model(self, filename):
        """Ladda modell fr√•n fil"""
        if self.model.load(filename):
            logging.info(f"üìÇ Modell laddad fr√•n {filename}")
            return True
        logging.warning(f"‚ö†Ô∏è Kunde inte ladda modell fr√•n {filename}")
        return False
    
    def save_stats(self, filename=None):
        """Spara tr√§ningsstatistik till fil"""
        if filename is None:
            filename = f"data/stats/training_stats_{int(time.time())}.json"
        
        # Konvertera numpy arrays till listor f√∂r JSON
        stats_json = {
            'episodes': self.training_stats['episodes'],
            'steps': self.training_stats['steps'],
            'rewards': [float(r) for r in self.training_stats['rewards']],
            'losses': [float(l) for l in self.training_stats['losses']],
            'epsilons': [float(e) for e in self.training_stats['epsilons']],
            'timestamp': datetime.now().isoformat()
        }
        
        with open(filename, 'w') as f:
            json.dump(stats_json, f, indent=2)
        
        logging.info(f"üìä Tr√§ningsstatistik sparad till {filename}")
    
    def load_stats(self, filename):
        """Ladda tr√§ningsstatistik fr√•n fil"""
        if os.path.exists(filename):
            with open(filename, 'r') as f:
                stats = json.load(f)
            
            self.training_stats['episodes'] = stats['episodes']
            self.training_stats['steps'] = stats['steps']
            self.training_stats['rewards'] = stats['rewards']
            self.training_stats['losses'] = stats['losses']
            self.training_stats['epsilons'] = stats['epsilons']
            
            logging.info(f"üìä Tr√§ningsstatistik laddad fr√•n {filename}")
            return True
        
        logging.warning(f"‚ö†Ô∏è Kunde inte ladda statistik fr√•n {filename}")
        return False
    
    def save_buffer(self, filename=None):
        """Spara replay buffer till fil"""
        if filename is None:
            filename = f"data/models/replay_buffer_{int(time.time())}.pkl"
        
        self.replay_buffer.save(filename)
        logging.info(f"üíæ Replay buffer sparad till {filename}")
    
    def load_buffer(self, filename):
        """Ladda replay buffer fr√•n fil"""
        self.replay_buffer.load(filename)
        logging.info(f"üìÇ Replay buffer laddad fr√•n {filename}")
    
    def get_q_values(self, state):
        """H√§mta Q-v√§rden f√∂r alla handlingar i ett givet tillst√•nd"""
        return self.model.predict(state)
    
    def get_state_features(self, screen_info, active_window, ocr_text):
        """
        Extrahera state features fr√•n sk√§rminformation
        
        Detta √§r en f√∂renklad implementation. I en faktisk implementation
        skulle denna metod extrahera meningsfulla funktioner fr√•n sk√§rmdata.
        """
        # Exempel p√• feature extraction
        features = np.zeros(self.state_dim)
        
        # Detta √§r bara platsh√•llare. En riktig implementation skulle
        # extrahera meningsfulla funktioner fr√•n sk√§rmdata, aktivt f√∂nster och OCR-text.
        
        return features